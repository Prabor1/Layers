# Layers_creation

Here I impelment my own custom layer. The layer is already available in Keras. I implemented Leaky ReLU using basic tf libraries. The data is popular mnist data, which is available with tf. The utils file will import the data and do the preprocessing for us. It also consists some helper function. After training model with our layer, I compared my result with ReLU, which gives a good result with overall 97% acc on val set. Source code for python is available.
